import cv2
from PIL import Image
import numpy as np
from ultralytics import YOLO
import gradio as gr
from openai import OpenAI
import time

# YOLO ëª¨ë¸ ë¡œë“œ
model = YOLO('yolo12x.pt')

# xAI API ì„¤ì •
client = OpenAI(
    api_key="API Key",  # ìœ íš¨í•œ xAI API í‚¤ë¡œ êµì²´
    base_url="https://api.x.ai/v1",  # xAI ë¬¸ì„œì—ì„œ í™•ì¸
)

# ê°ì²´ ì„¤ëª… í•¨ìˆ˜ (Grok 3 ì‚¬ìš©)
def describe_object(object_name):
    """Grok 3ë¥¼ ì‚¬ìš©í•´ ê°ì²´ ì„¤ëª…ì„ ìƒì„±."""
    prompt = f"Describe in one sentence what a {object_name} is and what it is used for."
    try:
        response = client.chat.completions.create(
            model="grok-3",  # xAI ë¬¸ì„œì—ì„œ ëª¨ë¸ ì´ë¦„ í™•ì¸
            messages=[{"role": "user", "content": prompt}]
        )
        description = response.choices[0].message.content.strip()
        return description
    except Exception as e:
        print(f"Grok API error: {e}")
        return f"{object_name}: [description not available]"

# ì›¹ìº  í”„ë ˆì„ ì²˜ë¦¬ í•¨ìˆ˜
def process_webcam_frame(frame):
    """ì›¹ìº  í”„ë ˆì„(BGR)ì„ ì²˜ë¦¬í•˜ê³  ê°ì²´ë¥¼ ê°ì§€í•˜ë©° ì„¤ëª…ì„ ìƒì„±."""
    if frame is None:
        print("Frame is None - possible causes: webcam not active, permissions denied, or no capture.")
        return None, "Please ensure webcam is active and permissions are granted."
    
    print(f"Webcam frame type: {type(frame)}, Shape: {getattr(frame, 'shape', 'N/A')}")
    
    try:
        # Webcam input is BGR, convert to RGB
        image = Image.fromarray(frame[..., ::-1])  # BGR -> RGB
    except Exception as e:
        print(f"Image conversion error: {e}")
        return None, "Invalid image format."
    
    try:
        results = model(image)
        print("Object detection completed.")
        annotated = results[0].plot()  # Annotated image (BGR)
        if annotated is None:
            print("Annotated image is None - detection failed.")
            return None, "Detection failed."
        
        # Convert annotated image from BGR to RGB
        annotated_rgb = cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB)
    except Exception as e:
        print(f"Detection error: {e}")
        return None, "Error during object detection."
    
    descriptions = {}
    for box in results[0].boxes:
        try:
            cls_id = int(box.cls[0])
            label = results[0].names[cls_id]
            if label not in descriptions:
                desc = describe_object(label)
                descriptions[label] = f"**{label.capitalize()}** â€“ {desc}"
                print(f"Detected: {label}, Description: {desc}")
        except Exception as e:
            print(f"Error processing detection: {e}")
            continue
    descriptions_text = "\n\n".join(descriptions.values()) if descriptions else "No objects detected."
    
    return annotated_rgb, descriptions_text

# ì—…ë¡œë“œ ì´ë¯¸ì§€ ì²˜ë¦¬ í•¨ìˆ˜
def process_uploaded_image(frame):
    """ì—…ë¡œë“œ ì´ë¯¸ì§€(RGB)ë¥¼ ì²˜ë¦¬í•˜ê³  ê°ì²´ë¥¼ ê°ì§€í•˜ë©° ì„¤ëª…ì„ ìƒì„±."""
    if frame is None:
        print("Frame is None - invalid upload.")
        return None, "Please upload a valid image."
    
    print(f"Uploaded image type: {type(frame)}, Shape: {getattr(frame, 'shape', 'N/A')}")
    
    try:
        # Uploaded image is RGB
        image = Image.fromarray(np.array(frame)).convert('RGB')
    except Exception as e:
        print(f"Image conversion error: {e}")
        return None, "Invalid image format."
    
    try:
        results = model(image)
        print("Object detection completed.")
        annotated = results[0].plot()  # Annotated image (BGR)
        if annotated is None:
            print("Annotated image is None - detection failed.")
            return None, "Detection failed."
        
        # Convert annotated image from BGR to RGB
        annotated_rgb = cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB)
    except Exception as e:
        print(f"Detection error: {e}")
        return None, "Error during object detection."
    
    descriptions = {}
    for box in results[0].boxes:
        try:
            cls_id = int(box.cls[0])
            label = results[0].names[cls_id]
            if label not in descriptions:
                desc = describe_object(label)
                descriptions[label] = f"**{label.capitalize()}** â€“ {desc}"
                print(f"Detected: {label}, Description: {desc}")
        except Exception as e:
            print(f"Error processing detection: {e}")
            continue
    descriptions_text = "\n\n".join(descriptions.values()) if descriptions else "No objects detected."
    
    return annotated_rgb, descriptions_text

# ì›¹ìº  í”„ë ˆì„ ìº¡ì²˜ í•¨ìˆ˜
def capture_webcam_frame():
    """ì›¹ìº ì—ì„œ ë‹¨ì¼ í”„ë ˆì„ì„ ìº¡ì²˜."""
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("Failed to open webcam.")
        return None, "Webcam could not be opened."
    
    ret, frame = cap.read()
    cap.release()
    if ret:
        return process_webcam_frame(frame)
    return None, "Failed to capture frame."

# Gradio ì¸í„°í˜ì´ìŠ¤
with gr.Blocks() as demo:
    gr.Markdown("## ğŸ‘“ AI Smart Glasses â€“ Object Detection Demo\nUpload an image or use the webcam button for detection.")
    with gr.Row():
        with gr.Column():
            input_image = gr.Image(sources=["webcam", "upload"], label="Camera Input", interactive=True, width=640, height=480)
            webcam_button = gr.Button("Capture from Webcam")
        with gr.Column():
            output_video = gr.Image(label="Detected Images")
            output_text = gr.Markdown(label="Object Descriptions")
    
    # ì›¹ìº  ë²„íŠ¼ ì´ë²¤íŠ¸
    webcam_button.click(
        fn=capture_webcam_frame,
        inputs=None,
        outputs=[output_video, output_text]
    )
    
    # ì´ë¯¸ì§€ ì—…ë¡œë“œ ì´ë²¤íŠ¸
    input_image.change(
        fn=process_uploaded_image,
        inputs=input_image,
        outputs=[output_video, output_text]
    )

# ì•± ì‹¤í–‰ (ë¡œì»¬ í™˜ê²½)
demo.launch(share=True)
